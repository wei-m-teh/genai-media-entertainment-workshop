{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75eb74dc-bb08-4f0d-a81e-bc87c4388ceb",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Question & Answering with SageMaker Jumpstart Foundation Model using LangChain and Amazon OpenSearch Serverless\n",
    "\n",
    "### Context\n",
    "Previously we showed how you could build a movie assistant AI RAG Chatbot using Knowledge Bases for Bedrock. In this notebook, we are going to explore another option of building a RAG chatbot using an LLM (Meta Llama2) hosted in Amazon SageMaker through SageMaker Jumpstart.\n",
    "\n",
    "\n",
    "### Architecture\n",
    "![qna-rag](images/langchain-sagemaker-qa-rag.png)\n",
    "\n",
    "### Challenges\n",
    "- How to manage large document(s) that could potentially exceed the token limit\n",
    "- How to find the document(s) relevant to the question being asked\n",
    "\n",
    "### Proposal\n",
    "To the above challenges, this notebook proposes the following strategy:\n",
    "\n",
    "#### Prepare documents\n",
    "![Embeddings](./images/embeddings_lang.png)\n",
    "\n",
    "Before being able to answer the questions, the documents must be processed and a stored in a document store index\n",
    "- Load the documents\n",
    "- Process and split them into smaller chunks\n",
    "- Create a numerical vector representation of each chunk using Amazon Bedrock Titan Embeddings model\n",
    "- Create an index using the chunks and the corresponding embeddings\n",
    "  \n",
    "#### Ask question\n",
    "![Question](./images/chatbot_lang.png)\n",
    "\n",
    "When the documents index is prepared, you are ready to ask the questions and relevant documents will be fetched based on the question being asked. Following steps will be executed.\n",
    "- Create an embedding of the input question\n",
    "- Compare the question embedding with the embeddings in the index\n",
    "- Fetch the (top N) relevant document chunks\n",
    "- Add those chunks as part of the context in the prompt\n",
    "- Send the prompt to the model hosted in SageMaker\n",
    "- Get the contextual answer based on the documents retrieved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfe68ce-bb5f-4335-9a2b-929c8225afdb",
   "metadata": {},
   "source": [
    "## Usecase\n",
    "#### Dataset\n",
    "To explain this architecture pattern we are using a few documents from MovieLens dataset. These documents explain topics such as:\n",
    "- Movie synopsis.\n",
    "- Release dates\n",
    "- Cast members\n",
    "  \n",
    "\n",
    "#### Persona\n",
    "Let's assume a persona of a user who is looking for information about movies/shows. \n",
    "\n",
    "The model will try to answer from the documents in easy language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbee1c-6c65-42c6-9d89-1d5561d485b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install opensearch-py==2.4.2 langchain==0.1.9 boto3 lark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe83d0a-dff2-4cf3-8224-6cf5c1e6b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets==8.0.4 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36c4b2d-24ce-4c10-beb1-28ab0c755391",
   "metadata": {},
   "source": [
    "# Langchain Integration \n",
    "<img src=\"images/langchain-logo.png\" alt=\"langchain\" style=\"width: 400px;\"/>\n",
    "LangChain is a framework for developing applications powered by LLMs. As a high level, langchain enables applications that are:\n",
    "\n",
    "* Data-aware: connect a language model to other sources of data\n",
    "* Agentic: allow a language model to interact with its environment\n",
    "\n",
    "The main advantages of using LangChain are:\n",
    "\n",
    "* Provides framework abstractions for working with language models, along with a collection of implementations for each abstraction. \n",
    "* Modular design principle promotes flexibility to use any LangChain components to build an application \n",
    "* Provides many Off-the-shelf chains that makes it easy to get started. \n",
    "\n",
    "Langchain also has robust Sagemaker support. In this workshop, we'll be using the following langchain components to integrate with the LLM model and the embeddings model deployed in SageMaker to build a simple Q&A application.\n",
    "\n",
    "\n",
    "* [Langchain SageMaker Endpoint](https://python.langchain.com/docs/integrations/providers/sagemaker_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03e09c-5bc1-4b5a-961f-deb305f14fad",
   "metadata": {},
   "source": [
    "Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8eca61-fede-4903-a094-d3c25cb52cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2ac0f-aa80-41ae-aaec-3206ba53bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import glob\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from typing import Any, Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f4bf2-4061-40a0-8d7b-79e817ce267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_id = str(uuid.uuid4().hex)[:5]\n",
    "vectordb_name=\"sm-llm-vector-db\"\n",
    "vector_store_name = f'{vectordb_name}-{random_id}'\n",
    "index_name = f\"{vectordb_name}-index-{random_id}\"\n",
    "encryption_policy_name = f\"{vectordb_name}-sp-{random_id}\"\n",
    "network_policy_name = f\"{vectordb_name}-np-{random_id}\"\n",
    "access_policy_name = f\"{vectordb_name}-ap-{random_id}\"\n",
    "kb_role_name = f\"{vectordb_name}-role-{random_id}\"\n",
    "knowledge_base_name = f\"{vectordb_name}-{random_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b16ed7-98fc-4387-9386-ad46626cf0c4",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "In the following section, we're going to prepare our knoledge base store using Amazon OpenSearch Severless collection. The dataset is provided in the 'data' folder in this project and ready to be ingested. We'll leverage langchain framework to help us simplify the data ingestion process.\n",
    "The main steps for data ingestion workflow are:\n",
    "\n",
    "1. Create an opensearch serverless collection\n",
    "2. Prepare documents from the data folder\n",
    "3. Creates an Embedding model to be used for converting the texts into vectors embeddings.\n",
    "4. Ingest the embeddings into the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fec2d7-dd3d-4ac9-b895-6d943a273034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opensearch_serverless_collection(vector_store_name, \n",
    "                                            index_name, \n",
    "                                            encryption_policy_name, \n",
    "                                            network_policy_name, \n",
    "                                            access_policy_name):\n",
    "    identity = boto3.client('sts').get_caller_identity()['Arn']\n",
    "\n",
    "    aoss_client = boto3.client('opensearchserverless')\n",
    "\n",
    "    security_policy = aoss_client.create_security_policy(\n",
    "        name = encryption_policy_name,\n",
    "        policy = json.dumps(\n",
    "            {\n",
    "                'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                'ResourceType': 'collection'}],\n",
    "                'AWSOwnedKey': True\n",
    "            }),\n",
    "        type = 'encryption'\n",
    "    )\n",
    "\n",
    "    network_policy = aoss_client.create_security_policy(\n",
    "        name = network_policy_name,\n",
    "        policy = json.dumps(\n",
    "            [\n",
    "                {'Rules': [{'Resource': ['collection/' + vector_store_name],\n",
    "                'ResourceType': 'collection'}],\n",
    "                'AllowFromPublic': True}\n",
    "            ]),\n",
    "        type = 'network'\n",
    "    )\n",
    "\n",
    "    collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')\n",
    "\n",
    "    while True:\n",
    "        status = aoss_client.list_collections(collectionFilters={'name':vector_store_name})['collectionSummaries'][0]['status']\n",
    "        if status in ('ACTIVE', 'FAILED'): break\n",
    "        time.sleep(10)\n",
    "\n",
    "    access_policy = aoss_client.create_access_policy(\n",
    "        name = access_policy_name,\n",
    "        policy = json.dumps(\n",
    "            [\n",
    "                {\n",
    "                    'Rules': [\n",
    "                        {\n",
    "                            'Resource': ['collection/' + vector_store_name],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateCollectionItems',\n",
    "                                'aoss:DeleteCollectionItems',\n",
    "                                'aoss:UpdateCollectionItems',\n",
    "                                'aoss:DescribeCollectionItems'],\n",
    "                            'ResourceType': 'collection'\n",
    "                        },\n",
    "                        {\n",
    "                            'Resource': ['index/' + vector_store_name + '/*'],\n",
    "                            'Permission': [\n",
    "                                'aoss:CreateIndex',\n",
    "                                'aoss:DeleteIndex',\n",
    "                                'aoss:UpdateIndex',\n",
    "                                'aoss:DescribeIndex',\n",
    "                                'aoss:ReadDocument',\n",
    "                                'aoss:WriteDocument'],\n",
    "                            'ResourceType': 'index'\n",
    "                        }],\n",
    "                    'Principal': [identity],\n",
    "                    'Description': 'Easy data policy'}\n",
    "            ]),\n",
    "        type = 'data'\n",
    "    )\n",
    "    collection_id = collection['createCollectionDetail']['id']\n",
    "    collection_arn = collection['createCollectionDetail']['arn']\n",
    "    host = collection['createCollectionDetail']['id'] + '.' + os.environ.get(\"AWS_DEFAULT_REGION\", None) + '.aoss.amazonaws.com'\n",
    "    return host, collection_id, collection_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e021e-cfc2-44f4-9af4-9da291c06fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "host, collection_id, collection_arn = create_opensearch_serverless_collection(vector_store_name,\n",
    "                                                                              index_name,\n",
    "                                                                              encryption_policy_name,\n",
    "                                                                              network_policy_name,\n",
    "                                                                              access_policy_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15528c2d-fa0a-4056-b9b2-f0ed2da6242a",
   "metadata": {},
   "source": [
    "## Creates documents and ingest into the opensearch serverless cluster.\n",
    "First, we iterate through the documents in the 'data' folder and create a Document object for each txt file. \n",
    "Then we feed the documents to opensearch serverless for ingestion using an `OpenSearchVectorSearch` object supported by Langchain framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88243dc0-9442-4048-b7e4-45a6437e4fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for file in glob.glob(f\"data/*.txt\"): \n",
    "    with open(file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "    movie_id = lines[0].split(\":\")[1].strip()\n",
    "    title = lines[1].split(\":\")[1].strip()\n",
    "    genres = lines[2].split(\":\")[1].strip()\n",
    "    spoken_languages = lines[3].split(\":\")[1].strip()\n",
    "    release_date = lines[4].split(\":\")[1].strip()\n",
    "    rating = lines[5].split(\":\")[1].strip()\n",
    "    if rating == \"nan\":\n",
    "        rating = \"0\"\n",
    "    cast = lines[6].split(\":\")[1].strip()\n",
    "    overview = lines[7].split(\":\")[1].strip()\n",
    "    doc = Document(\n",
    "        page_content=f\"{''.join(lines)}\",\n",
    "        metadata={\n",
    "            \"movie_id\": movie_id,\n",
    "            \"rating\": float(rating),\n",
    "            \"genres\": genres.split(\",\"),\n",
    "            \"spoken_languages\": spoken_languages.split(\",\"),\n",
    "            \"release_date\": release_date,\n",
    "            \"cast\" : cast.split(\",\")}\n",
    "        )\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8ad25-d2e5-475b-9519-fedd1d79c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3_credentials = boto3.Session().get_credentials() # needed for authenticating against opensearch cluster for index creation\n",
    "region = boto3.client(\"sts\").meta.region_name\n",
    "service = \"aoss\"\n",
    "auth = AWSV4SignerAuth(boto3_credentials, region, service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f44dea-36b7-4c2b-9e02-f5af0da0b38d",
   "metadata": {},
   "source": [
    "Define an embedding model and an LLM. In our example, we'll use Amazon Titan Embedding model as the embedding model, and Llama2-7b chat model hosted in Amazon SageMaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f307b-c778-4d26-848e-13f981669dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "\n",
    "embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fc2d71-3168-432b-9b71-11c9d3402883",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = OpenSearchVectorSearch.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    index_name=\"opensearch-self-query-demo\",\n",
    "    opensearch_url=f\"{host}:443\",\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    timeout = 100,\n",
    "    engine=\"faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ecc59-8ee2-4f09-92c4-326b69bc93b5",
   "metadata": {},
   "source": [
    "Let's validate the vectorDB by using a vector store retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bfb87-9b78-4d94-9ef3-15b497db7147",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_documents = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3}).get_relevant_documents(\"I want to watch an action movie with friendship and murder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32367c-9e1e-4fa7-a743-8cebec56820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in relevant_documents:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d5da7-e45b-478e-9e7f-cf360ad2fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instructions(instructions: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format instructions where conversation roles must alternate user/assistant/user/assistant/...\"\"\"\n",
    "    prompt: List[str] = []\n",
    "    for user, answer in zip(instructions[::2], instructions[1::2]):\n",
    "        prompt.extend([\"<s>\", \"[INST] \", (user[\"content\"]).strip(), \" [/INST] \", (answer[\"content\"]).strip(), \"</s>\"])\n",
    "    prompt.extend([\"<s>\", \"[INST] \", (instructions[-1][\"content\"]).strip(), \" [/INST] \"])\n",
    "    return \"\".join(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cdc352-d92a-4942-8284-e01921b987ee",
   "metadata": {},
   "source": [
    "Define a ContentHandler class for langchain LLM integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc743ba4-b5e0-4582-b07e-e19fc88e9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.sagemaker_endpoint import LLMContentHandler\n",
    "\n",
    "class SMLLMContentHandler(LLMContentHandler):\n",
    "        content_type = \"application/json\"\n",
    "        accepts = \"application/json\"\n",
    "\n",
    "        def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "            input_data = json.dumps([[{\"role\" : \"system\", \"content\" : \"You are a movie assistant.\"},\n",
    "                                    {\"role\" : \"user\", \"content\" : prompt}]])\n",
    "            input_str = json.dumps({\"inputs\" : input_data, \"parameters\" : {**model_kwargs}})\n",
    "            return input_str.encode('utf-8')\n",
    "\n",
    "        def transform_output(self, output: bytes) -> str:\n",
    "            response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "            return response_json[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4403ed24-99a6-4c8f-a157-3b565f508ea3",
   "metadata": {},
   "source": [
    "TODO: Need to add screenshots for creating the SageMaker endpoint from Jumpstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbd8944-6565-4ace-805f-22d03347b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_endpoint_name = \"jumpstart-llama2-7b-chat\"\n",
    "llm_inference_component_name = \"meta-textgeneration-llama-2-7b-f-20240223-235028\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ecdfa0-4f36-473b-a21a-0a12c578e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import SagemakerEndpoint\n",
    "\n",
    "region_name = \"us-east-1\"\n",
    "model_params = { \n",
    "                    \"do_sample\": True,\n",
    "                    \"top_p\": 0.9,\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"max_new_tokens\": 1000,\n",
    "                    \"stop\": [\"<|endoftext|>\", \"</s>\"],\n",
    "                    \"repetition_penalty\": 1.1\n",
    "               }\n",
    "\n",
    "llm = SagemakerEndpoint(\n",
    "    endpoint_name=llm_endpoint_name,\n",
    "    region_name=region_name,\n",
    "    content_handler = SMLLMContentHandler(),\n",
    "    model_kwargs = model_params,\n",
    "    endpoint_kwargs = {\"InferenceComponentName\" : llm_inference_component_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5fe5e-316d-4a38-ba7e-aad33711c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt_template = \"\"\"Given the following context and conversation history:\n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "\n",
    "Conversation History: \n",
    "{chat_history}\n",
    "\n",
    "Answer the question as truthfully as possible. Your answer must only be coming from the context given above. If the answer is not found in the given context. Say \"I don't know\"\n",
    "Your answer must be in a summary and direct in a concise manner. It's critical that you are only allowed to use the context given to you in answering the question.\n",
    "\n",
    "User question: {question}\"\"\".strip()\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"chat_history\", \"context\", \"question\"]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer')\n",
    "\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "    memory=memory,\n",
    "    retriever=vectorstore.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 3}), \n",
    "    return_source_documents=True,\n",
    "    combine_docs_chain_kwargs={\"prompt\": PROMPT})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc22ad9-8271-403a-89c3-5c85beac2adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Which animation movies contain toys in the plot?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5632b48e-9395-4355-9088-c2288498190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is Woody?\"\n",
    "result = qa({\"question\" : query})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649af0b3-97fa-490a-af21-4fa51618e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, doc in enumerate(result['source_documents']):\n",
    "    print(f\"=== doc {idx+1} ====\")\n",
    "    print(doc.page_content.replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1881e-7489-4cc6-8e41-b1270b9dcebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer')\n",
    "        self.qa = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "            memory=memory,\n",
    "            retriever=vectorstore.as_retriever(\n",
    "                search_type=\"similarity\", search_kwargs={\"k\": 3}), \n",
    "            return_source_documents=True,\n",
    "            combine_docs_chain_kwargs={\"prompt\": PROMPT})\n",
    "\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.out = ipw.Output()\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Let's chat!\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else:\n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            print(\"Thank you , that was a nice chat !!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=f\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    response = self.qa({\"question\" : prompt})\n",
    "                    result = response['answer']\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                print(f\"AI: {result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You: \", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246e1cd8-d2a5-491b-a962-7ceb30af1883",
   "metadata": {},
   "source": [
    "## Sample Questions\n",
    "* What's the movie \"Jumanji\" all about?\n",
    "* When was this movie released?\n",
    "* Who were the actors in this movie?\n",
    "* What movie would you recommend me watch after watching this movie?\n",
    "* What other movies were released in the same year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dfa90a-e1e3-4fda-b6f7-7771115e021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatUX()\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4031d0-d826-4e12-9d15-e7a1a3ec77ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "user-env:(genai-workshop)",
   "language": "python",
   "name": "genai-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
